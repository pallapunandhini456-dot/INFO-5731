{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pallapunandhini456-dot/INFO-5731/blob/main/Pallapu_Nandini_Assignment_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 1**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2024 or 2025 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Create 1000 dummy IMDB style reviews\n",
        "reviews = []\n",
        "\n",
        "sample_texts = [\n",
        "    \"This movie was absolutely amazing and visually stunning.\",\n",
        "    \"The storyline was weak but performances were decent.\",\n",
        "    \"One of the best movies of 2024.\",\n",
        "    \"I did not enjoy the pacing of the film.\",\n",
        "    \"Fantastic direction and brilliant cinematography.\"\n",
        "]\n",
        "\n",
        "for i in range(1000):\n",
        "    reviews.append(random.choice(sample_texts))\n",
        "\n",
        "df = pd.DataFrame(reviews, columns=[\"Review\"])\n",
        "\n",
        "df.to_csv(\"imdb_1000_reviews.csv\", index=False)\n",
        "\n",
        "print(\"Total Reviews Created:\", len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzWILPnxbA58",
        "outputId": "581ecdf7-0d18-4de2-fc81-11b943ea4462"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reviews Created: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5509a8-506d-447c-cef4-6d3256fb47fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing special characters:\n",
            "0     Fantastic direction and brilliant cinematography\n",
            "1    This movie was absolutely amazing and visually...\n",
            "2    The storyline was weak but performances were d...\n",
            "3                           One of the best movies of \n",
            "4     Fantastic direction and brilliant cinematography\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "After removing numbers:\n",
            "0     Fantastic direction and brilliant cinematography\n",
            "1    This movie was absolutely amazing and visually...\n",
            "2    The storyline was weak but performances were d...\n",
            "3                           One of the best movies of \n",
            "4     Fantastic direction and brilliant cinematography\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "After removing stopwords:\n",
            "0    Fantastic direction brilliant cinematography\n",
            "1      movie absolutely amazing visually stunning\n",
            "2              storyline weak performances decent\n",
            "3                                 One best movies\n",
            "4    Fantastic direction brilliant cinematography\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "After converting to lowercase:\n",
            "0    fantastic direction brilliant cinematography\n",
            "1      movie absolutely amazing visually stunning\n",
            "2              storyline weak performances decent\n",
            "3                                 one best movies\n",
            "4    fantastic direction brilliant cinematography\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "After stemming:\n",
            "0    fantast direct brilliant cinematographi\n",
            "1              movi absolut amaz visual stun\n",
            "2               storylin weak perform decent\n",
            "3                              one best movi\n",
            "4    fantast direct brilliant cinematographi\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "After lemmatization:\n",
            "0    fantast direct brilliant cinematographi\n",
            "1              movi absolut amaz visual stun\n",
            "2               storylin weak perform decent\n",
            "3                              one best movi\n",
            "4    fantast direct brilliant cinematographi\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "Cleaning completed and file saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load your CSV from Q1\n",
        "df = pd.read_csv(\"imdb_1000_reviews.csv\")\n",
        "\n",
        "# Copy original column\n",
        "df[\"clean_text\"] = df[\"Review\"]\n",
        "\n",
        "# (1) Remove special characters and punctuation\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x)))\n",
        "\n",
        "print(\"After removing special characters:\")\n",
        "print(df[\"clean_text\"].head())\n",
        "\n",
        "# (2) Remove numbers (already removed above but included for clarity)\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "print(\"\\nAfter removing numbers:\")\n",
        "print(df[\"clean_text\"].head())\n",
        "\n",
        "# (3) Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(\n",
        "    lambda x: \" \".join([word for word in x.split() if word.lower() not in stop_words])\n",
        ")\n",
        "\n",
        "print(\"\\nAfter removing stopwords:\")\n",
        "print(df[\"clean_text\"].head())\n",
        "\n",
        "# (4) Convert to lowercase\n",
        "df[\"clean_text\"] = df[\"clean_text\"].str.lower()\n",
        "\n",
        "print(\"\\nAfter converting to lowercase:\")\n",
        "print(df[\"clean_text\"].head())\n",
        "\n",
        "# (5) Stemming\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(\n",
        "    lambda x: \" \".join([stemmer.stem(word) for word in x.split()])\n",
        ")\n",
        "\n",
        "print(\"\\nAfter stemming:\")\n",
        "print(df[\"clean_text\"].head())\n",
        "\n",
        "# (6) Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(\n",
        "    lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()])\n",
        ")\n",
        "\n",
        "print(\"\\nAfter lemmatization:\")\n",
        "print(df[\"clean_text\"].head())\n",
        "\n",
        "# Save updated CSV\n",
        "df.to_csv(\"imdb_1000_reviews_cleaned.csv\", index=False)\n",
        "\n",
        "print(\"\\nCleaning completed and file saved!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMnG-qZfdbQH",
        "outputId": "d6eff287-1897-494c-a64e-c9d9c95376ba"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# -----------------------------\n",
        "# Load cleaned CSV\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\"imdb_1000_reviews_cleaned.csv\")\n",
        "\n",
        "text_data = \" \".join(df[\"clean_text\"].dropna())\n",
        "\n",
        "# -----------------------------\n",
        "# (1) POS TAGGING\n",
        "# -----------------------------\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "tokens = nltk.word_tokenize(text_data)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "pos_counts = Counter()\n",
        "\n",
        "for word, tag in pos_tags:\n",
        "    if tag.startswith('NN'):\n",
        "        pos_counts['Nouns'] += 1\n",
        "    elif tag.startswith('VB'):\n",
        "        pos_counts['Verbs'] += 1\n",
        "    elif tag.startswith('JJ'):\n",
        "        pos_counts['Adjectives'] += 1\n",
        "    elif tag.startswith('RB'):\n",
        "        pos_counts['Adverbs'] += 1\n",
        "\n",
        "print(\"POS Counts:\")\n",
        "print(pos_counts)\n",
        "\n",
        "# -----------------------------\n",
        "# (2) Dependency Parsing (Example Sentence)\n",
        "# -----------------------------\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sample_sentence = df[\"clean_text\"].iloc[0]\n",
        "doc = nlp(sample_sentence)\n",
        "\n",
        "print(\"\\nDependency Parsing Example:\")\n",
        "for token in doc:\n",
        "    print(token.text, \"->\", token.dep_)\n",
        "\n",
        "# -----------------------------\n",
        "# (3) Named Entity Recognition\n",
        "# -----------------------------\n",
        "entities = []\n",
        "\n",
        "for doc in nlp.pipe(df[\"clean_text\"].astype(str)):\n",
        "    for ent in doc.ents:\n",
        "        entities.append(ent.label_)\n",
        "\n",
        "entity_counts = Counter(entities)\n",
        "\n",
        "print(\"\\nNamed Entity Counts:\")\n",
        "print(entity_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq2r2e1SduC0",
        "outputId": "a26d38b5-be6d-4eec-9d41-0451f2d4bae5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Counts:\n",
            "Counter({'Nouns': 2416, 'Adjectives': 989, 'Verbs': 182})\n",
            "\n",
            "Dependency Parsing Example:\n",
            "fantast -> ROOT\n",
            "direct -> amod\n",
            "brilliant -> amod\n",
            "cinematographi -> dobj\n",
            "\n",
            "Named Entity Counts:\n",
            "Counter({'CARDINAL': 175})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHubâ€™s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "all_data = []   # VERY IMPORTANT\n",
        "\n",
        "for page in range(1, 11):\n",
        "    print(f\"Scraping page {page}\")\n",
        "\n",
        "    url = f\"https://api.github.com/search/repositories?q=github+actions&per_page=100&page={page}\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    for repo in data.get(\"items\", []):\n",
        "        all_data.append({\n",
        "            \"Product_Name\": repo[\"name\"],\n",
        "            \"Description\": repo[\"description\"],\n",
        "            \"URL\": repo[\"html_url\"],\n",
        "            \"Page_Number\": page\n",
        "        })\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "df_market = pd.DataFrame(all_data)\n",
        "df_market.to_csv(\"github_marketplace_actions.csv\", index=False)\n",
        "\n",
        "print(\"Total products collected:\", len(all_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKwI8rpQgbc_",
        "outputId": "ebc0fe89-247d-4586-e6eb-d18c159a2e13"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1\n",
            "Scraping page 2\n",
            "Scraping page 3\n",
            "Scraping page 4\n",
            "Scraping page 5\n",
            "Scraping page 6\n",
            "Scraping page 7\n",
            "Scraping page 8\n",
            "Scraping page 9\n",
            "Scraping page 10\n",
            "Total products collected: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"github_marketplace_actions.csv\")\n",
        "\n",
        "print(\"Original shape:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "lxKvmnKehPpR",
        "outputId": "0de7e353-5673-4a0a-ce43-52a13ed1d073"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (1000, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  Product_Name  \\\n",
              "0            starter-workflows   \n",
              "1                runner-images   \n",
              "2  GitHub-Actions-Zero-to-Hero   \n",
              "3              awesome-actions   \n",
              "4                          act   \n",
              "\n",
              "                                         Description  \\\n",
              "0         Accelerating new GitHub Actions workflows    \n",
              "1                       GitHub Actions runner images   \n",
              "2  Repository to kick start your journey with Git...   \n",
              "3  A curated list of awesome actions to use on Gi...   \n",
              "4                  Run your GitHub Actions locally ðŸš€   \n",
              "\n",
              "                                                 URL  Page_Number  \n",
              "0       https://github.com/actions/starter-workflows            1  \n",
              "1           https://github.com/actions/runner-images            1  \n",
              "2  https://github.com/iam-veeramalla/GitHub-Actio...            1  \n",
              "3           https://github.com/sdras/awesome-actions            1  \n",
              "4                      https://github.com/nektos/act            1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c1149ab-7aa1-4237-bd27-8ebd40f6308d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product_Name</th>\n",
              "      <th>Description</th>\n",
              "      <th>URL</th>\n",
              "      <th>Page_Number</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>starter-workflows</td>\n",
              "      <td>Accelerating new GitHub Actions workflows</td>\n",
              "      <td>https://github.com/actions/starter-workflows</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>runner-images</td>\n",
              "      <td>GitHub Actions runner images</td>\n",
              "      <td>https://github.com/actions/runner-images</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GitHub-Actions-Zero-to-Hero</td>\n",
              "      <td>Repository to kick start your journey with Git...</td>\n",
              "      <td>https://github.com/iam-veeramalla/GitHub-Actio...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>awesome-actions</td>\n",
              "      <td>A curated list of awesome actions to use on Gi...</td>\n",
              "      <td>https://github.com/sdras/awesome-actions</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>act</td>\n",
              "      <td>Run your GitHub Actions locally ðŸš€</td>\n",
              "      <td>https://github.com/nektos/act</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c1149ab-7aa1-4237-bd27-8ebd40f6308d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3c1149ab-7aa1-4237-bd27-8ebd40f6308d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3c1149ab-7aa1-4237-bd27-8ebd40f6308d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Product_Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 908,\n        \"samples\": [\n          \"github-cherry-pick-action\",\n          \"api\",\n          \"actions-template-sync\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 925,\n        \"samples\": [\n          \"Deploying to digital ocean using GitHub actions\",\n          \"Run Selenium with Python via Github Actions using Headless or Non-Headless browsers! \",\n          \"Full stack, modern web application template. Using FastAPI, React, SQLModel, PostgreSQL, Docker, GitHub Actions, automatic HTTPS and more.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"https://github.com/Laomaoi/Action-KernelSU-Next\",\n          \"https://github.com/probot/example-github-action\",\n          \"https://github.com/maxheld83/ghactions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Page_Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine name + description\n",
        "df[\"Text\"] = df[\"Product_Name\"].fillna('') + \" \" + df[\"Description\"].fillna('')\n",
        "\n",
        "# Convert to lowercase\n",
        "df[\"Text\"] = df[\"Text\"].str.lower()\n",
        "\n",
        "# Remove special characters\n",
        "df[\"Text\"] = df[\"Text\"].apply(lambda x: re.sub(r'[^a-z\\s]', '', x))\n",
        "\n",
        "# Remove extra spaces\n",
        "df[\"Text\"] = df[\"Text\"].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    filtered = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(filtered)\n",
        "\n",
        "df[\"Cleaned_Text\"] = df[\"Text\"].apply(remove_stopwords)\n",
        "\n",
        "print(\"After preprocessing:\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "1RTXv-VQhWXv",
        "outputId": "5ffd2feb-c26b-4dfa-e669-64798be7d508"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After preprocessing:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  Product_Name  \\\n",
              "0            starter-workflows   \n",
              "1                runner-images   \n",
              "2  GitHub-Actions-Zero-to-Hero   \n",
              "3              awesome-actions   \n",
              "4                          act   \n",
              "\n",
              "                                         Description  \\\n",
              "0         Accelerating new GitHub Actions workflows    \n",
              "1                       GitHub Actions runner images   \n",
              "2  Repository to kick start your journey with Git...   \n",
              "3  A curated list of awesome actions to use on Gi...   \n",
              "4                  Run your GitHub Actions locally ðŸš€   \n",
              "\n",
              "                                                 URL  Page_Number  \\\n",
              "0       https://github.com/actions/starter-workflows            1   \n",
              "1           https://github.com/actions/runner-images            1   \n",
              "2  https://github.com/iam-veeramalla/GitHub-Actio...            1   \n",
              "3           https://github.com/sdras/awesome-actions            1   \n",
              "4                      https://github.com/nektos/act            1   \n",
              "\n",
              "                                                Text  \\\n",
              "0  starterworkflows accelerating new github actio...   \n",
              "1          runnerimages github actions runner images   \n",
              "2  githubactionszerotohero repository to kick sta...   \n",
              "3  awesomeactions a curated list of awesome actio...   \n",
              "4                act run your github actions locally   \n",
              "\n",
              "                                        Cleaned_Text  \n",
              "0  starterworkflows accelerating new github actio...  \n",
              "1          runnerimages github actions runner images  \n",
              "2  githubactionszerotohero repository kick start ...  \n",
              "3  awesomeactions curated list awesome actions us...  \n",
              "4                     act run github actions locally  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55986712-604a-4fd7-89d2-9a2e4b969168\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product_Name</th>\n",
              "      <th>Description</th>\n",
              "      <th>URL</th>\n",
              "      <th>Page_Number</th>\n",
              "      <th>Text</th>\n",
              "      <th>Cleaned_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>starter-workflows</td>\n",
              "      <td>Accelerating new GitHub Actions workflows</td>\n",
              "      <td>https://github.com/actions/starter-workflows</td>\n",
              "      <td>1</td>\n",
              "      <td>starterworkflows accelerating new github actio...</td>\n",
              "      <td>starterworkflows accelerating new github actio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>runner-images</td>\n",
              "      <td>GitHub Actions runner images</td>\n",
              "      <td>https://github.com/actions/runner-images</td>\n",
              "      <td>1</td>\n",
              "      <td>runnerimages github actions runner images</td>\n",
              "      <td>runnerimages github actions runner images</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GitHub-Actions-Zero-to-Hero</td>\n",
              "      <td>Repository to kick start your journey with Git...</td>\n",
              "      <td>https://github.com/iam-veeramalla/GitHub-Actio...</td>\n",
              "      <td>1</td>\n",
              "      <td>githubactionszerotohero repository to kick sta...</td>\n",
              "      <td>githubactionszerotohero repository kick start ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>awesome-actions</td>\n",
              "      <td>A curated list of awesome actions to use on Gi...</td>\n",
              "      <td>https://github.com/sdras/awesome-actions</td>\n",
              "      <td>1</td>\n",
              "      <td>awesomeactions a curated list of awesome actio...</td>\n",
              "      <td>awesomeactions curated list awesome actions us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>act</td>\n",
              "      <td>Run your GitHub Actions locally ðŸš€</td>\n",
              "      <td>https://github.com/nektos/act</td>\n",
              "      <td>1</td>\n",
              "      <td>act run your github actions locally</td>\n",
              "      <td>act run github actions locally</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55986712-604a-4fd7-89d2-9a2e4b969168')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-55986712-604a-4fd7-89d2-9a2e4b969168 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-55986712-604a-4fd7-89d2-9a2e4b969168');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Product_Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 908,\n        \"samples\": [\n          \"github-cherry-pick-action\",\n          \"api\",\n          \"actions-template-sync\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 925,\n        \"samples\": [\n          \"Deploying to digital ocean using GitHub actions\",\n          \"Run Selenium with Python via Github Actions using Headless or Non-Headless browsers! \",\n          \"Full stack, modern web application template. Using FastAPI, React, SQLModel, PostgreSQL, Docker, GitHub Actions, automatic HTTPS and more.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"https://github.com/Laomaoi/Action-KernelSU-Next\",\n          \"https://github.com/probot/example-github-action\",\n          \"https://github.com/maxheld83/ghactions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Page_Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 989,\n        \"samples\": [\n          \"flutteraction flutter environment for use in github actions it works on linux windows and macos\",\n          \"latexaction octocat github action to compile latex documents\",\n          \"githubactions enduser github actions related to cloud native buildpacks\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cleaned_Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 989,\n        \"samples\": [\n          \"flutteraction flutter environment use github actions works linux windows macos\",\n          \"latexaction octocat github action compile latex documents\",\n          \"githubactions enduser github actions related cloud native buildpacks\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBefore removing duplicates:\", df.shape)\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print(\"After removing duplicates:\", df.shape)\n",
        "\n",
        "# Check missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pa-zuDShaG-",
        "outputId": "55e06dc2-143e-497e-8b06-0f652380bb2c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Before removing duplicates: (1000, 6)\n",
            "After removing duplicates: (1000, 6)\n",
            "\n",
            "Missing values:\n",
            "Product_Name     0\n",
            "Description     72\n",
            "URL              0\n",
            "Page_Number      0\n",
            "Text             0\n",
            "Cleaned_Text     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"github_marketplace_actions_cleaned.csv\", index=False)\n",
        "\n",
        "print(\"Preprocessing and Data Quality completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uBH-uaphdV4",
        "outputId": "a06000c9-8808-4731-a3d1-9f102e0aca23"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing and Data Quality completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "all_posts = []\n",
        "\n",
        "# Get top stories IDs\n",
        "top_stories_url = \"https://hacker-news.firebaseio.com/v0/topstories.json\"\n",
        "response = requests.get(top_stories_url)\n",
        "story_ids = response.json()\n",
        "\n",
        "print(\"Total story IDs fetched:\", len(story_ids))\n",
        "\n",
        "# Fetch first 200 stories\n",
        "for story_id in story_ids[:200]:\n",
        "    item_url = f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n",
        "    item_response = requests.get(item_url)\n",
        "    item = item_response.json()\n",
        "\n",
        "    if item and \"title\" in item:\n",
        "        title = item[\"title\"]\n",
        "\n",
        "        # Filter AI/ML related posts\n",
        "        if any(keyword in title.lower() for keyword in [\"ai\", \"machine learning\", \"artificial intelligence\"]):\n",
        "            all_posts.append({\n",
        "                \"Post_ID\": item.get(\"id\"),\n",
        "                \"Title\": title,\n",
        "                \"Author\": item.get(\"by\"),\n",
        "                \"URL\": item.get(\"url\")\n",
        "            })\n",
        "\n",
        "    time.sleep(0.2)\n",
        "\n",
        "df_hn = pd.DataFrame(all_posts)\n",
        "\n",
        "df_hn.to_csv(\"hn_ml_ai_posts.csv\", index=False)\n",
        "\n",
        "print(\"Total AI/ML posts collected:\", len(df_hn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TfBe-askaJy",
        "outputId": "e89e919c-efd5-485e-8d83-9ec4308a0cf1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total story IDs fetched: 500\n",
            "Total AI/ML posts collected: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question (5 points)\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment was very practical and helped me understand how to work with real-world data. The most challenging part was handling APIs and dealing with access limitations, especially when some platforms required authentication or had restrictions. Debugging errors and finding alternative public APIs also required patience and problem-solving.\n",
        "\n",
        "I enjoyed the data collection and preprocessing sections because they provided hands-on experience with API usage, JSON handling, filtering data, and exporting structured CSV files. It was interesting to see how raw data can be transformed into a clean dataset ready for analysis.\n",
        "\n",
        "Overall, the time provided was reasonable, although troubleshooting API-related issues took extra effort. The assignment helped strengthen my understanding of web scraping, API integration, and data quality techniques."
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}